{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификатор ориентации документов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Цель работы** — разработать решение для классификации ориентации документов на фото для последующей интеграции в готовый микросервис по их OCR-распознаванию. Классификатор должен определять одну из четырёх ориентаций для осуществления при необходимости соответствующего приведения изображения к классической ориентации. Данная операция необходима, так как модель сервиса для последующего распознавания справок работает только с изображениями с указанной ориентацией. Модель же, поддерживающая одновременно все из них, показала в среднем худшее качество распознавания текста по сравнению с исходной.\n",
    "\n",
    "В наличии имеется по 344 копии различных медицинских справок в каждой из 4 ориентаций.\n",
    "\n",
    "**Ход работы**\n",
    "\n",
    "Для начала определимся с методом получения векторных представлений, эмбеддингов входящих изображений, подготовим их.\n",
    "\n",
    "Далее выберем непосредственно классификатор, обучим его на имеющихся данных. После чего в итоге оценим качество его работы на тестовой выборке.\n",
    " \n",
    "Таким образом работа будет состоять из следующих этапов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Получение-эмбеддингов-изображений\" data-toc-modified-id=\"Получение-эмбеддингов-изображений-1\">Получение эмбеддингов изображений</a></span></li><li><span><a href=\"#Обучение-классификатора\" data-toc-modified-id=\"Обучение-классификатора-2\">Обучение классификатора</a></span></li><li><span><a href=\"#Проверка-качества-на-тестовых-данных\" data-toc-modified-id=\"Проверка-качества-на-тестовых-данных-3\">Проверка качества на тестовых данных</a></span></li><li><span><a href=\"#Вывод\" data-toc-modified-id=\"Вывод-4\">Вывод</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение эмбеддингов изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZTpI4KpCYI8v"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import plotly.io as pio\n",
    "from plotly.figure_factory import create_annotated_heatmap\n",
    "\n",
    "from sklearn.metrics import \\\n",
    "    f1_score, \\\n",
    "    roc_auc_score, \\\n",
    "    accuracy_score, \\\n",
    "    precision_score, \\\n",
    "    recall_score, \\\n",
    "    confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "from transformers import AutoModel\n",
    "\n",
    "from skorch.classifier import NeuralNetClassifier\n",
    "from skorch.callbacks import EpochScoring, EarlyStopping, LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "egf05w_bYI8w"
   },
   "outputs": [],
   "source": [
    "HOME = '/home/vladislav/ds/projects/pet/donorsearch_screenshots'\n",
    "pio.renderers.default = 'notebook_connected'\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "SEED = 345678\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала соберём датафрейм с путями к файлам и таргетом класса для получения эмбеддингов фотографий и их последующей классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>right/e2df6e921ec5453e8d00b34ca26948bc.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>left/2b123493ea3447e8a78cd77864e8dad8.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>right/d0407f45eeab4d048babd75cf9b87570.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>top/7cd69c4019a64858ba58dce8ed3ce1a8.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>left/912bf69198e945e3afd97a90839df4c7.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>bottom/e22a4589af944ede90750c5ab0a44b07.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>left/515f2081d16046ec94e59c6bcb1290e1.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>bottom/8d3fee13bd3f4bc58aed10b12986c5dc.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>top/788599452a194797a78525003923d5d4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>top/12bce2f3e2df49419b4f223574720dc2.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          images  label\n",
       "6     right/e2df6e921ec5453e8d00b34ca26948bc.jpg      1\n",
       "718    left/2b123493ea3447e8a78cd77864e8dad8.jpg      2\n",
       "459   right/d0407f45eeab4d048babd75cf9b87570.jpg      1\n",
       "129     top/7cd69c4019a64858ba58dce8ed3ce1a8.jpg      0\n",
       "149    left/912bf69198e945e3afd97a90839df4c7.jpg      2\n",
       "123  bottom/e22a4589af944ede90750c5ab0a44b07.jpg      3\n",
       "365    left/515f2081d16046ec94e59c6bcb1290e1.jpg      2\n",
       "177  bottom/8d3fee13bd3f4bc58aed10b12986c5dc.jpg      3\n",
       "401     top/788599452a194797a78525003923d5d4.jpg      0\n",
       "268     top/12bce2f3e2df49419b4f223574720dc2.jpg      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subfolders = ['top', 'right', 'left', 'bottom']\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "label_mapping = {'top': 0, 'right': 1, 'left': 2, 'bottom': 3}\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    path = os.path.join(HOME, subfolder)\n",
    "    for file in os.listdir(path):\n",
    "        images.append(os.path.join(subfolder, file))\n",
    "        labels.append(label_mapping[subfolder])\n",
    "\n",
    "orientations = pd.DataFrame({'images': images, 'label': labels}).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "orientations.sample(10, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переопределим класс Dataset из PyTorch для подгрузки изображений и напишем функцию для генерации эмбеддингов. Используем чуть больший размер изображения при масштабировании с помощью **Resize()**, чем при его окончательной обрезке, для сохранения большего количества информации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O8gqlksBYI88",
    "outputId": "e5761945-54b1-48de-f81b-aed69da00832"
   },
   "outputs": [],
   "source": [
    "class DFDataset(Dataset):\n",
    "    def __init__(self, df, dir, transform=None):\n",
    "        self.df = df\n",
    "        self.dir = dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file = row.images\n",
    "        path = os.path.join(self.dir, file)\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "def obtaining_img_embeddings(df, dir, model, batch_size):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(384),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5],\n",
    "            std=[0.5, 0.5, 0.5]\n",
    "            )\n",
    "        ])\n",
    "    dataset = DFDataset(df, dir, transform=preprocess)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    image_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            output = model(batch)\n",
    "            image_embeddings.append(output[0][:,0,:].numpy())\n",
    "            \n",
    "    return np.concatenate(image_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмбеддинги получим с помощью предобученной BEiT-модели (бертоподобного ViT) от Microsoft — https://huggingface.co/google/vit-base-patch16-224, во время экспериментов она лучше себя показала для данной задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "idD_d7b_YI8-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры тренировочной и тестовой выборок с эмбеддингами: (960, 768)(416, 768)\n",
      "Размеры тренировочной и тестовой выборок с целевым признаком: (960,)(416,)\n"
     ]
    }
   ],
   "source": [
    "beit = AutoModel.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n",
    "with open(os.path.join(HOME, 'beit_encoder.pkl'), 'wb') as f:\n",
    "    pickle.dump(beit, f)\n",
    "with open(os.path.join(HOME, 'beit_encoder.pkl'), 'rb') as f:\n",
    "    beit = pickle.load(f)\n",
    "\n",
    "base_ids = orientations['images'].apply(\n",
    "    lambda x: x.split('/')[-1].rsplit('.', 1)[0]\n",
    "    )\n",
    "train_ids, test_ids = train_test_split(\n",
    "    base_ids.unique(),\n",
    "    test_size=0.3,\n",
    "    random_state=SEED\n",
    "    )\n",
    "train_df = orientations[base_ids.isin(train_ids)]\n",
    "test_df = orientations[base_ids.isin(test_ids)]\n",
    "\n",
    "train_features = obtaining_img_embeddings(train_df, HOME, beit, 128)\n",
    "test_features = obtaining_img_embeddings(test_df, HOME, beit, 128)\n",
    "\n",
    "train_target = train_df['label'].values\n",
    "test_target = test_df['label'].values\n",
    "\n",
    "print(\n",
    "    'Размеры тренировочной и тестовой выборок с эмбеддингами: ',\n",
    "    train_features.shape,\n",
    "    test_features.shape,\n",
    "    '\\n',\n",
    "    'Размеры тренировочной и тестовой выборок с целевым признаком: ',\n",
    "    train_target.shape,\n",
    "    test_target.shape,\n",
    "    sep=''\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Окончательно подготовим данные к обучению, представив их как тензоры PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KXCWpE6sYI8_"
   },
   "outputs": [],
   "source": [
    "f_train = torch.FloatTensor(train_features)\n",
    "f_test = torch.FloatTensor(test_features)\n",
    "\n",
    "t_train = torch.LongTensor(train_target)\n",
    "t_test = torch.LongTensor(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение классификатора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве классификатора используем обычную FFNN c парой скрытых слоёв на базе классификатора из библиотеки Skorch. Значения различных гиперпараметров были подобраны ранее с помощью кросс-валидации.\n",
    "\n",
    "Ориентироваться при обучении в первую очередь будем на макро F1-меру, так как нам важно то, насколько качественно будет определён каждый класс по отдельности, потому что в дальнейшем в рамках работы всего микросервиса мы будем переворачивать изображение фиксированным образом в зависимости от предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OIVW1nr7YI9A"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_hidden_neurons_1=861,\n",
    "        n_hidden_neurons_2=141,\n",
    "        n_in_neurons=768,\n",
    "        n_out_neurons=4\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_in_neurons, n_hidden_neurons_1)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden_neurons_1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_hidden_neurons_1, n_hidden_neurons_2)\n",
    "        self.bn2 = nn.BatchNorm1d(n_hidden_neurons_2)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(n_hidden_neurons_2, n_out_neurons)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    module=Net,\n",
    "    verbose=0,\n",
    "    lr=5e-2,\n",
    "    batch_size=-1,\n",
    "    max_epochs=1000,\n",
    "    optimizer=optim.NAdam,\n",
    "    optimizer__eps=1e-06,\n",
    "    callbacks=[\n",
    "        EpochScoring(\n",
    "            scoring='f1_macro',\n",
    "            lower_is_better=False,\n",
    "            name='F1'\n",
    "            ),\n",
    "        EarlyStopping(\n",
    "            lower_is_better=False,\n",
    "            monitor='F1',\n",
    "            patience=55,\n",
    "            load_best=True\n",
    "            ),\n",
    "        LRScheduler(\n",
    "            policy=optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "            monitor='F1',\n",
    "            T_0=45\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(f_train, t_train)\n",
    "with open(os.path.join(HOME, 'skorch_orientation_classifier.pkl'), 'wb') as f:\n",
    "    pickle.dump(net, f)\n",
    "with open(os.path.join(HOME, 'skorch_orientation_classifier.pkl'), 'rb') as f:\n",
    "    net = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества на тестовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем как истинность прогнозов нашей модели для всех классов по отдельности, так и различные классификационные метрики на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"45f3b903-8ad7-444c-8161-efd9f8d36816\" class=\"plotly-graph-div\" style=\"height:500px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"45f3b903-8ad7-444c-8161-efd9f8d36816\")) {                    Plotly.newPlot(                        \"45f3b903-8ad7-444c-8161-efd9f8d36816\",                        [{\"colorscale\":[[0.0,\"rgb(0,0,255)\"],[1.0,\"rgb(255,0,0)\"]],\"reversescale\":false,\"showscale\":false,\"x\":[\"top\",\"right\",\"left\",\"bottom\"],\"y\":[\"top\",\"right\",\"left\",\"bottom\"],\"z\":[[100,0,1,3],[0,97,7,0],[0,15,89,0],[4,1,0,99]],\"type\":\"heatmap\"}],                        {\"annotations\":[{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"100\",\"x\":\"top\",\"xref\":\"x\",\"y\":\"top\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"0\",\"x\":\"right\",\"xref\":\"x\",\"y\":\"top\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"1\",\"x\":\"left\",\"xref\":\"x\",\"y\":\"top\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"3\",\"x\":\"bottom\",\"xref\":\"x\",\"y\":\"top\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"0\",\"x\":\"top\",\"xref\":\"x\",\"y\":\"right\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"97\",\"x\":\"right\",\"xref\":\"x\",\"y\":\"right\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"7\",\"x\":\"left\",\"xref\":\"x\",\"y\":\"right\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"0\",\"x\":\"bottom\",\"xref\":\"x\",\"y\":\"right\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"0\",\"x\":\"top\",\"xref\":\"x\",\"y\":\"left\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"15\",\"x\":\"right\",\"xref\":\"x\",\"y\":\"left\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"89\",\"x\":\"left\",\"xref\":\"x\",\"y\":\"left\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"0\",\"x\":\"bottom\",\"xref\":\"x\",\"y\":\"left\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"4\",\"x\":\"top\",\"xref\":\"x\",\"y\":\"bottom\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"1\",\"x\":\"right\",\"xref\":\"x\",\"y\":\"bottom\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"0\",\"x\":\"left\",\"xref\":\"x\",\"y\":\"bottom\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"99\",\"x\":\"bottom\",\"xref\":\"x\",\"y\":\"bottom\",\"yref\":\"y\"}],\"xaxis\":{\"dtick\":1,\"gridcolor\":\"rgb(0, 0, 0)\",\"side\":\"top\",\"ticks\":\"\",\"title\":{\"text\":\"\\u041f\\u0440\\u0435\\u0434\\u0441\\u043a\\u0430\\u0437\\u0430\\u043d\\u0438\\u044f\",\"standoff\":5}},\"yaxis\":{\"dtick\":1,\"ticks\":\"\",\"ticksuffix\":\"  \",\"title\":{\"text\":\"\\u0418\\u0441\\u0442\\u0438\\u043d\\u043d\\u044b\\u0435 \\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u044f\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"font\":{\"size\":15},\"title\":{\"text\":\"\\u041c\\u0430\\u0440\\u0438\\u0446\\u0430 \\u043e\\u0448\\u0438\\u0431\\u043e\\u043a \\u043d\\u0430 \\u0442\\u0435\\u0441\\u0442\\u043e\\u0432\\u044b\\u0445 \\u0434\\u0430\\u043d\\u043d\\u044b\\u0445\",\"x\":0.5,\"y\":0.95},\"width\":1000,\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('45f3b903-8ad7-444c-8161-efd9f8d36816');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision на тестовых данных 0.927\n",
      "Recall на тестовых данных 0.925\n",
      "F-1 на тестовых данных 0.926\n",
      "Accuracy на тестовых данных 0.925\n",
      "AUC-ROC на тестовых данных 0.984\n"
     ]
    }
   ],
   "source": [
    "preds = net.predict(f_test)\n",
    "conf_mx = pd.DataFrame(confusion_matrix(t_test, preds))\n",
    "\n",
    "fig = create_annotated_heatmap(\n",
    "    conf_mx.to_numpy().round(3),\n",
    "    x=['top', 'right', 'left', 'bottom'],\n",
    "    y=['top', 'right', 'left', 'bottom'],\n",
    "    colorscale='Bluered',\n",
    "    font_colors=['white']\n",
    "    ).update_layout(\n",
    "        font_size=15,\n",
    "        width=1000,\n",
    "        height=500,\n",
    "        xaxis_title='Предсказания',\n",
    "        yaxis_title='Истинные значения',\n",
    "        title=dict(\n",
    "            text='Марица ошибок на тестовых данных',\n",
    "            x=.5,\n",
    "            y=.95\n",
    "            )\n",
    "        ).update_xaxes(title_standoff=5)\n",
    "fig.show()\n",
    "print()\n",
    "print(\n",
    "    'Precision на тестовых данных',\n",
    "    precision_score(t_test, preds, average='macro').round(3)\n",
    "    )\n",
    "print(\n",
    "    'Recall на тестовых данных',\n",
    "    recall_score(t_test, preds, average='macro').round(3)\n",
    "    )\n",
    "print(\n",
    "    'F-1 на тестовых данных',\n",
    "    f1_score(t_test, preds, average='macro').round(3)\n",
    "    )\n",
    "print(\n",
    "    'Accuracy на тестовых данных',\n",
    "    accuracy_score(t_test, preds).round(3)\n",
    "    )\n",
    "print(\n",
    "    'AUC-ROC на тестовых данных',\n",
    "    roc_auc_score(\n",
    "        t_test,\n",
    "        net.predict_proba(f_test),\n",
    "        multi_class='ovr',\n",
    "        average='macro'\n",
    "        ).round(3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разработанный классификатор достаточно хорошо справляется с определением ориентации медицинских документов, показатели всех метрик от 0.925. В итоге в контексте работы сервиса только в районе 8% изображений будут ошибочно не обработаны итоговой моделью распознавания."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 26051,
    "start_time": "2023-03-25T10:53:17.432Z"
   },
   {
    "duration": 5253,
    "start_time": "2023-03-25T10:53:43.486Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-25T10:53:48.741Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-25T10:53:48.742Z"
   },
   {
    "duration": 4023,
    "start_time": "2023-03-25T10:54:15.582Z"
   },
   {
    "duration": 321,
    "start_time": "2023-03-25T11:01:03.624Z"
   },
   {
    "duration": 11430,
    "start_time": "2023-03-25T11:01:06.235Z"
   },
   {
    "duration": 46,
    "start_time": "2023-03-25T11:01:17.667Z"
   },
   {
    "duration": 15,
    "start_time": "2023-03-25T11:01:32.724Z"
   },
   {
    "duration": 34308,
    "start_time": "2023-03-25T11:01:33.944Z"
   },
   {
    "duration": 766,
    "start_time": "2023-03-25T11:02:08.253Z"
   },
   {
    "duration": 6170,
    "start_time": "2023-03-25T11:16:28.920Z"
   },
   {
    "duration": 5807,
    "start_time": "2023-03-25T11:24:09.573Z"
   },
   {
    "duration": 9688,
    "start_time": "2023-03-25T11:48:19.625Z"
   },
   {
    "duration": 91,
    "start_time": "2023-03-25T11:48:41.834Z"
   }
  ],
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Содержание",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.417px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "6fa0252278bce9d2aa0bf042564670db102350afa6b5651ecfaa0000c01c0491"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
