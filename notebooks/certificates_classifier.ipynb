{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификатор справок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Цель работы** — разработать решение для классификации медицинских справок для последующей интеграции в готовый микросервис по их OCR-распознаванию. Классификатор на данном этапе должен определять, 405 форма справки или же нет, в зависимости от чего будет решаться, передавать ли изображение на распознавание.\n",
    "\n",
    "Предоставлено различное количество справок 5 типов:\n",
    "\n",
    "* справки с Госуслуг\n",
    "* 402 форма\n",
    "* 406 форма\n",
    "* 448 форма\n",
    "* 405 форма (требуемая к распознаванию)\n",
    "\n",
    "**Ход работы**\n",
    "\n",
    "Для начала определимся с методом получения векторных представлений, эмбеддингов входящих изображений, подготовим их.\n",
    "\n",
    "Далее выберем непосредственно классификатор, обучим его на имеющихся данных. После чего в итоге оценим качество его работы на тестовой выборке.\n",
    " \n",
    "Таким образом работа будет состоять из следующих этапов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Получение-эмбеддингов-изображений\" data-toc-modified-id=\"Получение-эмбеддингов-изображений-1\">Получение эмбеддингов изображений</a></span></li><li><span><a href=\"#Обучение-классификатора\" data-toc-modified-id=\"Обучение-классификатора-2\">Обучение классификатора</a></span></li><li><span><a href=\"#Проверка-качества-на-тестовых-данных\" data-toc-modified-id=\"Проверка-качества-на-тестовых-данных-3\">Проверка качества на тестовых данных</a></span></li><li><span><a href=\"#Вывод\" data-toc-modified-id=\"Вывод-4\">Вывод</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение эмбеддингов изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZTpI4KpCYI8v"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import plotly.io as pio\n",
    "from plotly.figure_factory import create_annotated_heatmap\n",
    "\n",
    "from sklearn.metrics import \\\n",
    "    f1_score, \\\n",
    "    roc_auc_score, \\\n",
    "    accuracy_score, \\\n",
    "    precision_score, \\\n",
    "    recall_score, \\\n",
    "    confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "from transformers import AutoModel\n",
    "\n",
    "from skorch.classifier import NeuralNetBinaryClassifier\n",
    "from skorch.callbacks import EpochScoring, EarlyStopping, LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "egf05w_bYI8w"
   },
   "outputs": [],
   "source": [
    "HOME = 'C:\\\\Users\\\\darve\\\\Pet_projects\\\\donorsearch_screenshots'\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "SEED = 345678\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала соберём датафрейм с путями к файлам и бинарным таргетом для получения эмбеддингов фотографий и последующей классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>is_405</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>402\\15df545bfeae45b2bae756282a8cb7ef11.large.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>448\\dc11323beb2d403382ebb9483965dda3.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>gos\\b541f70e3b4444b494dcf433a2730155.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>448\\708543765e494e55b041a3b94517d74a.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>448\\621433877a3643969f3c702947e56527.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>gos\\e5e68b1e804e4694a000fb58141d1d83.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>405\\2cb625f02fcb4e01abebb8855ae9eade.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>405\\94b271775ec04796a10e65a6cbb45370.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>gos\\a0d4ed12d90746b68c9e7ffe6cd0a3e4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>gos\\7d7b928a524a4a1f9a1281349a33b760.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               images  is_405\n",
       "7    402\\15df545bfeae45b2bae756282a8cb7ef11.large.jpg       0\n",
       "231          448\\dc11323beb2d403382ebb9483965dda3.jpg       0\n",
       "309          gos\\b541f70e3b4444b494dcf433a2730155.jpg       0\n",
       "203          448\\708543765e494e55b041a3b94517d74a.jpg       0\n",
       "200          448\\621433877a3643969f3c702947e56527.jpg       0\n",
       "331          gos\\e5e68b1e804e4694a000fb58141d1d83.jpg       0\n",
       "116          405\\2cb625f02fcb4e01abebb8855ae9eade.jpg       1\n",
       "141          405\\94b271775ec04796a10e65a6cbb45370.jpg       1\n",
       "303          gos\\a0d4ed12d90746b68c9e7ffe6cd0a3e4.jpg       0\n",
       "294          gos\\7d7b928a524a4a1f9a1281349a33b760.jpg       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subfolders = ['402', '405', '406', '448', 'gos']\n",
    "images = []\n",
    "targets = []\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    path = os.path.join(HOME, subfolder)\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.jpg'):\n",
    "            images.append(os.path.join(subfolder, file))\n",
    "            targets.append(int(subfolder == '405'))\n",
    "\n",
    "ocr = pd.DataFrame({'images': images, 'is_405': targets})\n",
    "ocr.sample(10, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переопределим класс Dataset из PyTorch для подгрузки изображений и напишем функцию для генерации эмбеддингов. Используем чуть больший размер изображения при масштабировании с помощью **Resize()**, чем при его окончательной обрезке, для сохранения большего количества информации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O8gqlksBYI88",
    "outputId": "e5761945-54b1-48de-f81b-aed69da00832"
   },
   "outputs": [],
   "source": [
    "class DFDataset(Dataset):\n",
    "    def __init__(self, df, dir, transform=None):\n",
    "        self.df = df\n",
    "        self.dir = dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file = row.images\n",
    "        path = os.path.join(self.dir, file)\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "def obtaining_img_embeddings(df, dir, model, batch_size):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(384),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5],\n",
    "            std=[0.5, 0.5, 0.5]\n",
    "            )\n",
    "        ])\n",
    "    dataset = DFDataset(df, dir, transform=preprocess)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    image_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            output = model(batch)\n",
    "            image_embeddings.append(output[0][:,0,:].numpy())\n",
    "            \n",
    "    return np.concatenate(image_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмбеддинги получим с помощью предобученной BEiT-модели (бертоподобного ViT) от Microsoft — https://huggingface.co/google/vit-base-patch16-224, во время экспериментов она лучше себя показала для данной задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/beit-base-patch16-224-pt22k-ft22k were not used when initializing BeitModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BeitModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BeitModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер эмбеддингов изображений:  (342, 768)\n"
     ]
    }
   ],
   "source": [
    "beit = AutoModel.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n",
    "with open(os.path.join(HOME, 'beit_encoder.pkl'), 'wb') as f:\n",
    "    pickle.dump(beit, f)\n",
    "with open(os.path.join(HOME, 'beit_encoder.pkl'), 'rb') as f:\n",
    "    beit = pickle.load(f)\n",
    "\n",
    "embeddings = obtaining_img_embeddings(\n",
    "    ocr,\n",
    "    HOME,\n",
    "    beit,\n",
    "    128\n",
    "    )\n",
    "print(\n",
    "    'Размер эмбеддингов изображений: ',\n",
    "    embeddings.shape\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Окончательно подготовим данные к обучению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "idD_d7b_YI8-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры тренировочной и тестовой выборок с эмбеддингами: (239, 768)(103, 768)\n",
      "Размеры тренировочной и тестовой выборок с целевым признаком: (239,)(103,)\n"
     ]
    }
   ],
   "source": [
    "target = ocr.is_405\n",
    "train_features, test_features, train_target, test_target = (\n",
    "    train_test_split(\n",
    "        embeddings,\n",
    "        target,\n",
    "        test_size=.3,\n",
    "        stratify=target,\n",
    "        random_state=SEED\n",
    "        )\n",
    "    )\n",
    "print(\n",
    "    'Размеры тренировочной и тестовой выборок с эмбеддингами: ',\n",
    "    train_features.shape,\n",
    "    test_features.shape,\n",
    "    '\\n',\n",
    "    'Размеры тренировочной и тестовой выборок с целевым признаком: ',\n",
    "    train_target.shape,\n",
    "    test_target.shape,\n",
    "    sep=''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KXCWpE6sYI8_"
   },
   "outputs": [],
   "source": [
    "f_train = torch.FloatTensor(train_features)\n",
    "f_test = torch.FloatTensor(test_features)\n",
    "\n",
    "t_train = torch.FloatTensor(train_target.values)\n",
    "t_test = torch.FloatTensor(test_target.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение классификатора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве классификатора используем обычную FFNN c одним скрытым слоем на базе бинарного классификатора из библиотеки Skorch. Значения различных гиперпараметров были подобраны ранее с помощью кросс-валидации.\n",
    "\n",
    "Ориентироваться при обучении в первую очередь будем на F1-меру, так как на данном этапе работы всего микросервиса нам одинаково важно как то, чтобы наибольшее количество 405-ых справок попало дальше на распознавание, так и то, чтобы наименьше возможное количество других справок в данном случае бесполезно нагружало наш сервис."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OIVW1nr7YI9A"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_hidden_neurons=138,\n",
    "        n_in_neurons=768,\n",
    "        n_out_neurons=1\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_in_neurons, n_hidden_neurons)\n",
    "        self.bn = nn.BatchNorm1d(n_hidden_neurons)\n",
    "        self.act = nn.ELU()\n",
    "        self.fc2 = nn.Linear(n_hidden_neurons, n_out_neurons)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = NeuralNetBinaryClassifier(\n",
    "    module=Net,\n",
    "    verbose=0,\n",
    "    lr=5e-2,\n",
    "    batch_size=-1,\n",
    "    max_epochs=100,\n",
    "    optimizer=optim.RAdam,\n",
    "    optimizer__eps=1e-06,\n",
    "    callbacks=[\n",
    "        EpochScoring(\n",
    "            scoring='f1',\n",
    "            lower_is_better=False,\n",
    "            name='F1'\n",
    "            ),\n",
    "        EarlyStopping(\n",
    "            lower_is_better=False,\n",
    "            monitor='F1',\n",
    "            patience=40,\n",
    "            load_best=True\n",
    "            ),\n",
    "        LRScheduler(\n",
    "            policy=optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "            monitor='F1',\n",
    "            T_0=10\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(f_train, t_train)\n",
    "with open(os.path.join(HOME, 'skorch_ffnn_classifier.pkl'), 'wb') as f:\n",
    "    pickle.dump(net, f)\n",
    "with open(os.path.join(HOME, 'skorch_ffnn_classifier.pkl'), 'rb') as f:\n",
    "    net = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества на тестовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем отдельно как все типы прогнозов нашей модели, так и различные классификационные метрики на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"136393f9-b531-4efc-aae3-dcc8550f1eb0\" class=\"plotly-graph-div\" style=\"height:500px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"136393f9-b531-4efc-aae3-dcc8550f1eb0\")) {                    Plotly.newPlot(                        \"136393f9-b531-4efc-aae3-dcc8550f1eb0\",                        [{\"colorscale\":[[0.0,\"rgb(0,0,255)\"],[1.0,\"rgb(255,0,0)\"]],\"reversescale\":false,\"showscale\":false,\"x\":[\"0\",\"1\"],\"y\":[\"1\",\"0\"],\"z\":[[79,1],[2,21]],\"type\":\"heatmap\"}],                        {\"annotations\":[{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"79\",\"x\":\"0\",\"xref\":\"x\",\"y\":\"1\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"1\",\"x\":\"1\",\"xref\":\"x\",\"y\":\"1\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"2\",\"x\":\"0\",\"xref\":\"x\",\"y\":\"0\",\"yref\":\"y\"},{\"font\":{\"color\":\"white\"},\"showarrow\":false,\"text\":\"21\",\"x\":\"1\",\"xref\":\"x\",\"y\":\"0\",\"yref\":\"y\"}],\"xaxis\":{\"dtick\":1,\"gridcolor\":\"rgb(0, 0, 0)\",\"side\":\"top\",\"ticks\":\"\",\"title\":{\"text\":\"\\u041f\\u0440\\u0435\\u0434\\u0441\\u043a\\u0430\\u0437\\u0430\\u043d\\u0438\\u044f\",\"standoff\":5}},\"yaxis\":{\"dtick\":1,\"ticks\":\"\",\"ticksuffix\":\"  \",\"title\":{\"text\":\"\\u0418\\u0441\\u0442\\u0438\\u043d\\u043d\\u044b\\u0435 \\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u044f\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"font\":{\"size\":15},\"title\":{\"text\":\"\\u041c\\u0430\\u0440\\u0438\\u0446\\u0430 \\u043e\\u0448\\u0438\\u0431\\u043e\\u043a \\u043d\\u0430 \\u0442\\u0435\\u0441\\u0442\\u043e\\u0432\\u044b\\u0445 \\u0434\\u0430\\u043d\\u043d\\u044b\\u0445\",\"x\":0.5,\"y\":0.95},\"width\":1000,\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('136393f9-b531-4efc-aae3-dcc8550f1eb0');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision на тестовых данных 0.955\n",
      "Recall на тестовых данных 0.913\n",
      "F-1 на тестовых данных 0.933\n",
      "Accuracy на тестовых данных 0.971\n",
      "AUC-ROC на тестовых данных 0.997\n"
     ]
    }
   ],
   "source": [
    "preds = net.predict(f_test)\n",
    "conf_mx = pd.DataFrame(confusion_matrix(t_test, preds))\n",
    "\n",
    "fig = create_annotated_heatmap(\n",
    "    conf_mx.to_numpy().round(3),\n",
    "    x = ['0', '1'],\n",
    "    y = ['1', '0'],\n",
    "    colorscale='Bluered',\n",
    "    font_colors=['white']\n",
    "    ).update_layout(\n",
    "        font_size=15,\n",
    "        width=1000,\n",
    "        height=500,\n",
    "        xaxis_title='Предсказания',\n",
    "        yaxis_title='Истинные значения',\n",
    "        title=dict(\n",
    "            text='Марица ошибок на тестовых данных',\n",
    "            x=.5,\n",
    "            y=.95\n",
    "            )\n",
    "        ).update_xaxes(title_standoff=5)\n",
    "fig.show()\n",
    "print()\n",
    "print(\n",
    "    'Precision на тестовых данных',\n",
    "    precision_score(t_test, preds).round(3)\n",
    "    )\n",
    "print(\n",
    "    'Recall на тестовых данных',\n",
    "    recall_score(t_test, preds).round(3)\n",
    "    )\n",
    "print(\n",
    "    'F-1 на тестовых данных',\n",
    "    f1_score(t_test, preds).round(3)\n",
    "    )\n",
    "print(\n",
    "    'Accuracy на тестовых данных',\n",
    "    accuracy_score(t_test, preds).round(3)\n",
    "    )\n",
    "print(\n",
    "    'AUC-ROC на тестовых данных',\n",
    "    roc_auc_score(\n",
    "        t_test,\n",
    "        net.predict_proba(f_test)[:, 1]\n",
    "        ).round(3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разработанный классификатор достаточно хорошо справляется с распознаванием справок 405 формы относительно остальных в имеющихся данных, показатели всех метрик выше 0.9. Однако в дальнейшем может потребоваться его доработка для мультиклассификации уже по всем типам справок."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 26051,
    "start_time": "2023-03-25T10:53:17.432Z"
   },
   {
    "duration": 5253,
    "start_time": "2023-03-25T10:53:43.486Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-25T10:53:48.741Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-25T10:53:48.742Z"
   },
   {
    "duration": 4023,
    "start_time": "2023-03-25T10:54:15.582Z"
   },
   {
    "duration": 321,
    "start_time": "2023-03-25T11:01:03.624Z"
   },
   {
    "duration": 11430,
    "start_time": "2023-03-25T11:01:06.235Z"
   },
   {
    "duration": 46,
    "start_time": "2023-03-25T11:01:17.667Z"
   },
   {
    "duration": 15,
    "start_time": "2023-03-25T11:01:32.724Z"
   },
   {
    "duration": 34308,
    "start_time": "2023-03-25T11:01:33.944Z"
   },
   {
    "duration": 766,
    "start_time": "2023-03-25T11:02:08.253Z"
   },
   {
    "duration": 6170,
    "start_time": "2023-03-25T11:16:28.920Z"
   },
   {
    "duration": 5807,
    "start_time": "2023-03-25T11:24:09.573Z"
   },
   {
    "duration": 9688,
    "start_time": "2023-03-25T11:48:19.625Z"
   },
   {
    "duration": 91,
    "start_time": "2023-03-25T11:48:41.834Z"
   }
  ],
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Содержание",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.417px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "6fa0252278bce9d2aa0bf042564670db102350afa6b5651ecfaa0000c01c0491"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
